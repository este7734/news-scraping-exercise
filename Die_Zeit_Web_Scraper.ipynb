{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Die_Zeit_Web_Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HcOlndfh9XwA"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/este7734/news-scraping-exercise/blob/master/Die_Zeit_Web_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWXpudN7R3Qc",
        "colab_type": "text"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_F9Q7Td-sRVi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1bed7bd1-293c-40e8-e540-7c5da32a4e59"
      },
      "source": [
        "# Import libraries for processing web text\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from lxml import html\n",
        "\n",
        "# Import these dependencies if using Google Colab \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLQoyopg1oiY",
        "colab_type": "text"
      },
      "source": [
        "## Define All Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "mh-Z_yZsENF2",
        "colab": {}
      },
      "source": [
        "# Get content of the webpage in an html string format by passing a url \n",
        "def get_html(url):\n",
        "    page = requests.get(url)\n",
        "    html_out = html.fromstring(page.content)\n",
        "    text = page.text\n",
        "    return html_out, text\n",
        "\n",
        "# Convert html into soup to enable soup menthods\n",
        "def get_soup(html_string):\n",
        "    soup = BeautifulSoup(html_string, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "# Extract hyperlinks from soup\n",
        "def get_soup_links(soup):\n",
        "    links = []\n",
        "    for link in soup.find_all('a'):\n",
        "        out_link = link.get('href')\n",
        "        links.append(out_link)\n",
        "    return links\n",
        "\n",
        "# This function is for use with only the Topic pages on reuters.com\n",
        "# Search through ALL links and filter for only those that are for actual articles\n",
        "# links are formatted differently \n",
        "def get_articles_reuters_topics(links, old_url_set):\n",
        "    articles = []\n",
        "    for link in links:\n",
        "        try:\n",
        "            split_link = link.split('/')\n",
        "            #if 'www.reuters.com'in split_link:\n",
        "            if 'www.spiegel.de'in split_link:\n",
        "              for topic in spiegel_topics:\n",
        "                if topic in split_link:\n",
        "                  link = link\n",
        "                  if url_check(old_url_set, link) == False:\n",
        "                    articles.append(link)\n",
        "        except:\n",
        "            continue\n",
        "    articles = list(set(articles))\n",
        "    old_url_set = set(articles + list(old_url_set))       \n",
        "    return articles, old_url_set\n",
        "\n",
        "# Check if new urls exists in the old_url_set. if yes, return True; if no, return False\n",
        "# This function is used in the get_articles_reuters_topics function\n",
        "def url_check(old_url_set, url):\n",
        "    url_set = set([url])\n",
        "    test_set = old_url_set & url_set\n",
        "    if len(test_set) == 0:\n",
        "        check = False\n",
        "    else:\n",
        "        check = True\n",
        "    return check\n",
        "\n",
        "# Get html strings from list of article weblinks\n",
        "def get_html_reuters(articles):\n",
        "    soup_list = []\n",
        "    for article in articles:\n",
        "        _, text = get_html(article)\n",
        "        soup = get_soup(text)\n",
        "        soup_list.append(soup)\n",
        "    return soup_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wVxrANXEF-W",
        "colab_type": "text"
      },
      "source": [
        "Tags for different websites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n28RUx9VH9sB",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Die Zeit classes and tags\n",
        "body_class = 'div'\n",
        "headline_class = 'h1'\n",
        "date_class = 'div'\n",
        "\n",
        "body_tag = 'article-body article-body--article'\n",
        "headline_tag = 'headline article__item'\n",
        "date_tag = 'metadata'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBmEliCH2G9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Break out article_body, article_headline, and article_date from each article in provided hyperlinks and put into a dictionary called: out_list\n",
        "def get_reuters_elements(soup_list, articles):\n",
        "    out_list = []\n",
        "    i = 0\n",
        "    for article in soup_list:\n",
        "        link = articles[i] # I don't think this is used at all here, which means there is no reason to require the second argument: articles\n",
        "        i += 1\n",
        "        try:\n",
        "            article_body = article.find_all(body_class, {'class': body_tag})\n",
        "            article_p = []\n",
        "            for item in article_body:\n",
        "                p_list = item.find_all('p')\n",
        "                for p in p_list:\n",
        "                    article_p.append(p.text)\n",
        "            out_text = ' '.join(article_p)\n",
        "            if out_text == '':\n",
        "              continue\n",
        "            if out_text.startswith('Besondere Reportagen'):\n",
        "              continue\n",
        "            out_dict = dict([('Text',out_text),('url',link)])\n",
        "            out_list.append(out_dict)\n",
        "        except:\n",
        "            print('Unable to decode...skipping article...')\n",
        "            continue\n",
        "\n",
        "    return out_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_qCIHy31uSJ",
        "colab_type": "text"
      },
      "source": [
        "## Define URL Variables and Run Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU0yQpNqLwl4",
        "colab_type": "text"
      },
      "source": [
        "Step 1. Instantiate `old_url_set` to be used in the `get_articles_reuters_topics` function. This is a running log of article links that will be compiled by iterating from steps 2 - 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S4mzVpzp5g1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate empty set to use a running list of hyperlinks while \n",
        "# running the scrape iterations\n",
        "old_url_set = set([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBkhWZ5iMWq9",
        "colab_type": "text"
      },
      "source": [
        "## Scrape Reuters Topics pages for all the most recent news articles. <font color='orange'>*Run Steps 2 - 7 for each instance of `url` variable, before moving on to the next steps*</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URLqBDPwUbRZ",
        "colab_type": "text"
      },
      "source": [
        "<font color='orange'>Step 2.</font> Define variables for each of Reuters main topics pages. Run this cell for each iteration by uncommenting a different url each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8a1cAaw3PfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define url variables\n",
        "# NOTE: You must run these individually through the end of this section\n",
        "# I didn't have time to figure out how to loop through all of them properly\n",
        "# There is a section at the very bottom where you can see that I attempted but ran\n",
        "# into a problem on one of the last functions. \n",
        "\n",
        "# Der Speigel Links\n",
        "#url = r'https://www.spiegel.de/'\n",
        "#url = r'https://www.spiegel.de/plus/'\n",
        "url = r'https://www.spiegel.de/schlagzeilen/'\n",
        "#url = r'https://www.spiegel.de/politik/deutschland/'\n",
        "#url = r'https://www.spiegel.de/politik/ausland/'\n",
        "#url = r'https://www.spiegel.de/panorama/'\n",
        "#url = r'https://www.spiegel.de/wirtschaft/'\n",
        "#url = r'https://www.spiegel.de/netzwelt/'\n",
        "#url = r'https://www.spiegel.de/wissenschaft/'\n",
        "#url = r'https://www.spiegel.de/geschichte/'\n",
        "#url = r'https://www.spiegel.de/thema/leben/'\n",
        "\n",
        "spiegel_topics = ['plus', 'schlagzeilen', 'politik/deutschland', 'politik/ausland', 'panorama', 'wirtschaft', 'netzwelt', 'wissenschaft', 'geschichte', 'thema/leben']\n",
        "\n",
        "#=====================================================================\n",
        "# Welt Links\n",
        "#url = r'https://www.welt.de/'\n",
        "#url = r'https://www.welt.de/weltplus/'\n",
        "#url = r'https://www.welt.de/politik/'\n",
        "#url = r'https://www.welt.de/wirtschaft/'\n",
        "#url = r'https://www.welt.de/vermischtes/'\n",
        "#url = r'https://www.welt.de/debatte/'\n",
        "\n",
        "# Die Zeit Links\n",
        "#url = r'https://www.zeit.de/index'\n",
        "#url = r'https://www.zeit.de/politik/index'\n",
        "#url = r'https://www.zeit.de/gesellschaft/index'\n",
        "#url = r'https://www.zeit.de/wirtschaft/index'\n",
        "#url = r'https://www.zeit.de/wissen/index'\n",
        "#url = r'https://www.zeit.de/digital/index'\n",
        "#url = r'https://www.zeit.de/entdecken/index'\n",
        "#url = r'https://www.zeit.de/zeit-magazin/index'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qGyTkHR3Tos",
        "colab_type": "text"
      },
      "source": [
        "## Scraper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctx4uUaNM-qy",
        "colab_type": "text"
      },
      "source": [
        "<font color='orange'>Step 3.</font> Get HTML srting from web `url`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJshIC2L4iPA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8569ac6d-f838-444c-9f9d-0a70dd44d3c5"
      },
      "source": [
        "# Pass the each instance of `url` variable to return the web page in HTML format and convert it to a string\n",
        "html_string = str(get_html(url))\n",
        "# Pass the HTML string (of the web page) to get its soup\n",
        "soup = get_soup(html_string)\n",
        "# Find ALL links on within the soup\n",
        "links = get_soup_links(soup)\n",
        "# Use this for Topics Pages only\n",
        "# Filter out only those links that are for actual articles. We only want the \"good\" links\n",
        "# This filters out things like links to images and advertisements or non-news worthy pages\n",
        "articles, old_url_set = get_articles_reuters_topics(links, old_url_set)\n",
        "print(len(articles))\n",
        "# Print out the running list of hyperlinks to see how many you have\n",
        "print(len(old_url_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383\n",
            "383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNqPBrCDCP4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linkers = list(old_url_set)\n",
        "linkers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWBBUihX8R0w",
        "colab_type": "text"
      },
      "source": [
        "## <font color='skyblue'>Parse soup from entire list of hyperlinks that you just accumulated</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5M5Po9_OWk2",
        "colab_type": "text"
      },
      "source": [
        "Step 8. Get soup for every link\n",
        "\n",
        "Step 9. Parse the soup for each link into `article_body`, `article_title`, and `article_date`. Create list of dictionaries for each web page\n",
        "\n",
        "Step 10. Run `TextBlob` on list of dictionaries to separate all sentences in a single list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZoenVkMIukU",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "354c89cc-0538-4842-fe28-edacf6ae10d8"
      },
      "source": [
        "# Get soup for each one of the \"good\" links\n",
        "url_links = list(old_url_set) # Convert the running set of links to a list for use in the following functions\n",
        "\n",
        "soup_list = get_html_reuters(url_links)\n",
        "print(f'Length of Soup List: {len(soup_list)}')\n",
        "#print(soup_list[0])\n",
        "\n",
        "# Parse the soup for each \"good\" link to get article text, title, and date\n",
        "out_list = get_reuters_elements(soup_list, url_links)\n",
        "print(f'Length of out_list: {len(out_list)}')\n",
        "#out_list[0:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Soup List: 383\n",
            "Length of out_list: 346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO7ZWsj3GFWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "727a21c4-1ad5-438c-a764-cabc5155389d"
      },
      "source": [
        "blob_sentences =[]\n",
        "for i in range(len(out_list)):\n",
        "  try:\n",
        "    blob = out_list[i]['Text']\n",
        "    trans = TextBlob(blob) # Enter string object\n",
        "    trans = trans.translate(to='en')\n",
        "    trans = str(trans) # Change to = 'en' for English\n",
        "    trans = TextBlob(trans)\n",
        "    for item in trans.sentences:\n",
        "      blob_sentences.append(trans)\n",
        "    #print('\\n', i+1, '\\n',trans)\n",
        "  except:\n",
        "    print('got an error ...., skipping article....')\n",
        "print(f'\\nYou have {len(blob_sentences)} total sentences from {i+1} different articles')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "You have 12262 total sentences from 346 different articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XjoXSZDPHLr",
        "colab_type": "text"
      },
      "source": [
        "This is just a troubleshooting section to ensure you're `TextBlob` came out right. You should see a list of stentences, each starting with the word `Sentence`. Check this before moving to the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4fJQzcl5X3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bb9b07e0-1dd9-4de8-d708-0597a0d4da71"
      },
      "source": [
        "# print the first 6 sentences so you see what it looks like\n",
        "blob_sentences[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"A thick cloud of Sahara sand has reached the Caribbean and is now moving towards the US coast, reports the US Weather Service. Measured by its size and concentration of dust, it is the largest cloud of its kind in half a century. The phenomenon is therefore known as the \"Godzilla\" cloud. Because of the dust particles, the air quality has already reached values ​​in some regions of the Caribbean from which experts assume a possible health hazard. People in affected regions are encouraged to stay indoors and use air filters if they have them. At nine million square kilometers, the Sahara is the largest dry desert on earth - and the largest source of dust on our planet. Every year, around 500 million tons of it fly from there to different parts of the world, sometimes thousands of kilometers away, including Germany. San Juan: The visibility at the airport was less than eight kilometers. It is normal for dust clouds to form over the Sahara at this time of year, which are carried westwards by winds. The extremely dry and dusty air moves across the North Atlantic approximately every three to five days from late spring to autumn, reports the U.S. Climate Change Agency. Dust transport across the Atlantic plays an important role for ecosystems. For example, sandstorms from the Sahara in the Amazon region fertilize the rainforest. In the Caribbean, almost the entire upper layer of the earth comes from Africa. During the summer months, the Sahara clouds move towards the Caribbean, in winter to the Amazon region. This time, however, the dust cloud is exceptionally large. Satellite images from NASA show how the cloud has spread since mid-June. In the meantime, it stretched more than 3,200 kilometers from the west coast of Africa to the edge of the Lesser Antilles in the western North Atlantic. The visibility at the airport of San Juan, the capital of Puerto Rico, was just eight kilometers. \"This is the most significant event in the past 50 years,\" said environmental scientist Pablo Méndez Lázaro from the University of Puerto Rico. Meanwhile, the cloud is moving toward the southeast coast of the United States.\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGJmYoaJYF_n",
        "colab_type": "text"
      },
      "source": [
        "## <font color='skyblue'> Keyword Search </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkcNzckEPdOf",
        "colab_type": "text"
      },
      "source": [
        "Step 11. Determine key words used to search for event of interest in all the articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xN0ZSBJ510j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "384ecce7-3fbb-4ad7-de98-23da4a04a1e6"
      },
      "source": [
        "# Allow user to type in key words to search the text for\n",
        "# Note this is case sensitive... so you need to make sure you enter your search \n",
        "# Try the entering the following to see some results: corona,COVID,death\n",
        "filter_list = input(\"Enter key words to search for separated by commas. don't use spaces.\\nSearch is case sensitive.\\nExample search: coronavirus,COVID,death\\n\\n\") #.title() # This is still a string... not a list yet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter key words to search for separated by commas. don't use spaces.\n",
            "Search is case sensitive.\n",
            "Example search: coronavirus,COVID,death\n",
            "\n",
            "America\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8asVgYSPsbu",
        "colab_type": "text"
      },
      "source": [
        "Step 12. Convert keywords into an iterable list for use in the next step\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49R1H35B7aH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "925d1540-d920-4497-d69f-756277473bcc"
      },
      "source": [
        "# Split filter words and convert into a list for itterating in the next step \n",
        "\n",
        "f = []\n",
        "for word in (filter_list.split(\",\")):  # Split string into separate words, separate by comma\n",
        "  f.append(word)                       # Generate new list containing each key word\n",
        "f # This is now a list of key words that the user typed in"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['America']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTNasDzQLinR",
        "colab_type": "text"
      },
      "source": [
        "Step 13. <font color='skyblue'> Search </font>  All sentences for Key Words and return only those consisting of a key word\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXjYMd1-LUrT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ffe4794e-5965-4048-edd0-64c09c1b55e3"
      },
      "source": [
        "# Instantiate list for holding the sentences\n",
        "sentences = []\n",
        "\n",
        "# Generate empty list of lists to store the sentences with your key words in them\n",
        "for i in range(len(f)):\n",
        "  sentences.append([])\n",
        "#print('Here is what you just made, and empty list of lists: ', sentences, '\\n')\n",
        "\n",
        "# Generate lists of sentences for each key word and plug them into the list of lists from above            \n",
        "for i in range(len(f)):\n",
        "  for sentence in blob_sentences:\n",
        "    if f[i] in sentence:\n",
        "        sentences[i].append(sentence)\n",
        "        \n",
        "# Print number of sentences containing each key word\n",
        "# Print out all sentences containing each key word\n",
        "for i in range(len(f)):\n",
        "  print('='*200)   \n",
        "  print('\\nThere are {} sentences containing the word: {} '.format(len(sentences[i]), f[i])) \n",
        "  print('-'*200)   \n",
        "  #for sentence in sentences[i]:\n",
        "      #print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================================================================================================================================================\n",
            "\n",
            "There are 3648 sentences containing the word: America \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcOlndfh9XwA",
        "colab_type": "text"
      },
      "source": [
        "## <font color='black'>Step 13. Store All article data in DataFrame </font>  \n",
        "<font color='grey'>Note: This is not just the filtered data, this contains all information from each web page. While not used in this project, it represents all of the original data used in this run. It can be used for reference.</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjj8Wi-4yOSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "6961d849-3032-4493-9491-f4c35bd282eb"
      },
      "source": [
        "# Put parsed data into Pandas DataFrame\n",
        "pd.DataFrame(out_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In den USA steigen die Zahlen der Neuinfektion...</td>\n",
              "      <td>https://www.spiegel.de/panorama/corona-pandemi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In Mexiko ist nach einem Erdbeben der Stärke 7...</td>\n",
              "      <td>https://www.spiegel.de/panorama/mexiko-erdbebe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kurz vor der Entscheidung der Lufthansa-Aktion...</td>\n",
              "      <td>https://www.spiegel.de/wirtschaft/unternehmen/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Die Corona-Zahlen sind in Deutschland seit der...</td>\n",
              "      <td>https://www.spiegel.de/wissenschaft/medizin/rk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nach den Krawallen in Stuttgart zeigen sich di...</td>\n",
              "      <td>https://www.spiegel.de/panorama/stuttgart-kraw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>Auch der Eiscremehersteller Ben &amp; Jerry's will...</td>\n",
              "      <td>https://www.spiegel.de/netzwelt/web/facebook-w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>Bis zum Ausbruch der Corona-Pandemie galt Wirt...</td>\n",
              "      <td>https://www.spiegel.de/wirtschaft/unternehmen/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>Der frühere Boxweltmeister Roberto Durán ist i...</td>\n",
              "      <td>https://www.spiegel.de/panorama/leute/roberto-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>Ron Jeremy, US-amerikanischer Pornodarsteller,...</td>\n",
              "      <td>https://www.spiegel.de/panorama/leute/ron-jere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>SPIEGEL: Herr Antinori, Sie haben mehr als ein...</td>\n",
              "      <td>https://www.spiegel.de/wirtschaft/unternehmen/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>346 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Text                                                url\n",
              "0    In den USA steigen die Zahlen der Neuinfektion...  https://www.spiegel.de/panorama/corona-pandemi...\n",
              "1    In Mexiko ist nach einem Erdbeben der Stärke 7...  https://www.spiegel.de/panorama/mexiko-erdbebe...\n",
              "2    Kurz vor der Entscheidung der Lufthansa-Aktion...  https://www.spiegel.de/wirtschaft/unternehmen/...\n",
              "3    Die Corona-Zahlen sind in Deutschland seit der...  https://www.spiegel.de/wissenschaft/medizin/rk...\n",
              "4    Nach den Krawallen in Stuttgart zeigen sich di...  https://www.spiegel.de/panorama/stuttgart-kraw...\n",
              "..                                                 ...                                                ...\n",
              "341  Auch der Eiscremehersteller Ben & Jerry's will...  https://www.spiegel.de/netzwelt/web/facebook-w...\n",
              "342  Bis zum Ausbruch der Corona-Pandemie galt Wirt...  https://www.spiegel.de/wirtschaft/unternehmen/...\n",
              "343  Der frühere Boxweltmeister Roberto Durán ist i...  https://www.spiegel.de/panorama/leute/roberto-...\n",
              "344  Ron Jeremy, US-amerikanischer Pornodarsteller,...  https://www.spiegel.de/panorama/leute/ron-jere...\n",
              "345  SPIEGEL: Herr Antinori, Sie haben mehr als ein...  https://www.spiegel.de/wirtschaft/unternehmen/...\n",
              "\n",
              "[346 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}